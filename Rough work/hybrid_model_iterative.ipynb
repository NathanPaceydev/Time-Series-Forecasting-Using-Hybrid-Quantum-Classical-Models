{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b17d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ba0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "# This is important for consistent results across runs\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # For GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Make operations deterministic (slower but reproducible)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbafd357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a window size (e.g., 20)\n",
    "WINDOW_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ff1d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 2 # number of qubits\n",
    "q_depth  = 2 # number of layers\n",
    "n_rot_params = 3  # <--- number of rotation parameters per qubit (e.g. 1 for RY, 3 for Rot)\n",
    "\n",
    "# wether to use quantum or classical model\n",
    "use_quantum = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd22dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_notes = \"test run without Quantum layer\"\n",
    "\n",
    "# === Model Config ===\n",
    "# Set model hyperparameters\n",
    "input_size = 1\n",
    "hidden_size = 16\n",
    "output_size = 1\n",
    "\n",
    "# draw the QML or ML model graph\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_epochs = 15\n",
    "use_dropout=False\n",
    "dropout_rate= 0.0 # 0.2 to 0.5 is common for LSTM/GRU\n",
    "use_layernorm=False \n",
    "\n",
    "# === Early Stopping Config ===\n",
    "early_stop_patience = 2 # Number of epochs to wait for improvement\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Type of ML unit to use\n",
    "ML_unit_type = \"LSTM\"  # \"LSTM\", \"GRU\", or \"RNN\" can havy many layers\n",
    "num_ML_layers = 4  # Number of LSTM/GRU/RNN layers\n",
    "\n",
    "post_quantum_activation = None  # \"ReLU\", \"Tanh\", \"Sigmoid\", or None\n",
    "skip_connection = \"concat\"  # \"concat\" or \"add\"\n",
    "output_activation = None # \"ReLU\", \"Tanh\", \"Sigmoid\", \"Softmax\", or None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79b6dc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading data for AAPL...\n",
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading data for MSFT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading data for GOOGL...\n",
      "✅ Loaded 3 tickers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ✅ List of tickers you want to load\n",
    "tickers = [\"AAPL\", \"MSFT\", \"GOOGL\"]\n",
    "\n",
    "# ✅ Dictionary to hold processed DataFrames per ticker\n",
    "ticker_data = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"📥 Downloading data for {ticker}...\")\n",
    "    \n",
    "    df = yf.download(ticker, start=\"2015-01-01\", end=\"2024-12-31\", interval=\"1d\")\n",
    "\n",
    "    # Flatten MultiIndex if necessary (e.g., from yf multi-ticker download)\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "    # Focus on 'Close' prices\n",
    "    df = df[['Close']].copy()\n",
    "    df.rename(columns={'Close': 'price'}, inplace=True)\n",
    "\n",
    "    # Forward-fill missing values\n",
    "    df.ffill(inplace=True)\n",
    "\n",
    "    # Normalize prices with individual scalers\n",
    "    scaler = MinMaxScaler()\n",
    "    df['normalized'] = scaler.fit_transform(df[['price']])\n",
    "\n",
    "    # Store\n",
    "    ticker_data[ticker] = df\n",
    "\n",
    "print(f\"✅ Loaded {len(ticker_data)} tickers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70fd642",
   "metadata": {},
   "source": [
    "**Data Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54e4f41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creating sequences for AAPL...\n",
      "✅ AAPL: Train=(1996, 20), Test=(499, 20)\n",
      "🔄 Creating sequences for MSFT...\n",
      "✅ MSFT: Train=(1996, 20), Test=(499, 20)\n",
      "🔄 Creating sequences for GOOGL...\n",
      "✅ GOOGL: Train=(1996, 20), Test=(499, 20)\n",
      "\n",
      "📦 Combined Dataset — Train: (5988, 20), Test: (1497, 20)\n"
     ]
    }
   ],
   "source": [
    "# To perform time series forecasting, we convert the normalized series into input/output sequences \n",
    "# using a sliding window approach.\n",
    "# [past inputs] → [future targets]\n",
    "def create_sequences(series, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - window_size):\n",
    "        X.append(series[i:i + window_size])\n",
    "        y.append(series[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Initialize holders for aggregated data\n",
    "X_train_all, y_train_all = [], []\n",
    "X_test_all, y_test_all = [], []\n",
    "\n",
    "# Apply to each ticker\n",
    "for ticker, df in ticker_data.items():\n",
    "    print(f\"🔄 Creating sequences for {ticker}...\")\n",
    "    series = df['normalized'].values\n",
    "    X, y = create_sequences(series, WINDOW_SIZE)\n",
    "\n",
    "    # Time-respecting 80/20 split\n",
    "    split_index = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    # Append to global containers\n",
    "    X_train_all.append(X_train)\n",
    "    y_train_all.append(y_train)\n",
    "    X_test_all.append(X_test)\n",
    "    y_test_all.append(y_test)\n",
    "\n",
    "    print(f\"✅ {ticker}: Train={X_train.shape}, Test={X_test.shape}\")\n",
    "\n",
    "# Combine across all tickers\n",
    "X_train = np.vstack(X_train_all)\n",
    "y_train = np.hstack(y_train_all)\n",
    "X_test = np.vstack(X_test_all)\n",
    "y_test = np.hstack(y_test_all)\n",
    "\n",
    "print(f\"\\n📦 Combined Dataset — Train: {X_train.shape}, Test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3522d47d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20526148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device definition\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_circuit(inputs, weights):\n",
    "    \"\"\"\n",
    "    Variational quantum circuit for hybrid model.\n",
    "    \n",
    "    Args:\n",
    "        inputs (Tensor): Input features (size ≤ n_qubits)\n",
    "        weights (Tensor): Trainable parameters of shape (q_depth, n_qubits, n_rot_params)\n",
    "        draw (bool): If True, draw the circuit once (ASCII + matplotlib)\n",
    "        \n",
    "    Returns:\n",
    "        List[Expectation values] for PauliZ on each qubit\n",
    "    \"\"\"\n",
    "    # --- Input Encoding ---\n",
    "    encoded_inputs = inputs[:n_qubits]\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(encoded_inputs[i], wires=i)\n",
    "    \n",
    "    # Optionally, use this instead:\n",
    "    # qml.templates.AngleEmbedding(inputs, wires=range(n_qubits), rotation='Y')\n",
    "\n",
    "    # --- Variational Layers ---\n",
    "    for layer in range(q_depth):\n",
    "        for i in range(n_qubits):\n",
    "            if n_rot_params == 1:\n",
    "                qml.RY(weights[layer][i][0], wires=i)\n",
    "            elif n_rot_params == 2:\n",
    "                qml.RX(weights[layer][i][0], wires=i)\n",
    "                qml.RZ(weights[layer][i][1], wires=i)\n",
    "            elif n_rot_params == 3:\n",
    "                qml.Rot(*weights[layer][i], wires=i)\n",
    "            else:\n",
    "                raise ValueError(\"n_rot_params must be 1, 2, or 3.\")\n",
    "\n",
    "        # Entanglement (ring topology)\n",
    "        for i in range(n_qubits):\n",
    "            qml.CNOT(wires=[i, (i + 1) % n_qubits])\n",
    "\n",
    "    \n",
    "\n",
    "    # --- Measurement ---\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0c7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fdd0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer(nn.Module):\n",
    "    def __init__(self, n_qubits, q_depth, n_rot_params):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize trainable parameters for the quantum circuit\n",
    "        # Shape: [q_depth, n_qubits, n_rot_params] (rotation angles per qubit per layer)\n",
    "        q_init = torch.empty(q_depth, n_qubits, n_rot_params)\n",
    "        torch.nn.init.normal_(q_init, mean=0.0, std=0.01)  # Small init to avoid flat gradients\n",
    "        self.q_params = nn.Parameter(q_init)\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.q_depth = q_depth\n",
    "        self.n_rot_params = n_rot_params\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \"\"\"\n",
    "        Apply the quantum circuit to each sample in the batch.\n",
    "\n",
    "        Args:\n",
    "            x_batch (Tensor): Input of shape [batch_size, n_qubits]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output of shape [batch_size, n_qubits]\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        for x in x_batch:\n",
    "            # Apply the quantum circuit to each sample\n",
    "            q_out = quantum_circuit(x, self.q_params)\n",
    "\n",
    "            # Convert list of expectation values into a float32 tensor\n",
    "            q_tensor = torch.stack(q_out).to(dtype=torch.float32)\n",
    "\n",
    "            outputs.append(q_tensor)\n",
    "\n",
    "        # Stack the results into a batch tensor\n",
    "        return torch.stack(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ad20f5",
   "metadata": {},
   "source": [
    "**Dummy Class for visualizing the Quantum Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d79a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb45b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridQNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                 n_qubits=4, q_depth=1, n_rot_params=3, ML_unit_type = 'LSTM', num_ML_layers = 1, use_quantum=True,\n",
    "                 post_quantum_activation=None, skip_connection=\"concat\",output_activation=\"Sigmoid\",\n",
    "                 use_dropout=False, dropout_rate=0.3,\n",
    "                 use_layernorm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_quantum = use_quantum\n",
    "        self.post_quantum_activation = post_quantum_activation  # 👈 Store this\n",
    "        self.skip_connection = skip_connection  # 👈 Store this\n",
    "        self.n_qubits = n_qubits  # 👈 Store this\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_layernorm = use_layernorm\n",
    "\n",
    "\n",
    "\n",
    "        ML_unit_type = ML_unit_type.upper()\n",
    "        # ✅ Check that it's at least valid before trying to build\n",
    "        if not any(unit in ML_unit_type for unit in [\"LSTM\", \"GRU\", \"RNN\"]):\n",
    "            raise ValueError(f\"Unsupported ML unit type: {ML_unit_type}. Supported types: LSTM, GRU, RNN\")\n",
    "\n",
    "        # Build stack\n",
    "        self.sequence_stack = nn.Sequential()\n",
    "\n",
    "        if \"LSTM\" in ML_unit_type:\n",
    "            self.sequence_stack.add_module(\"lstm\", nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                                                        num_layers=num_ML_layers, batch_first=True))\n",
    "\n",
    "        if \"GRU\" in ML_unit_type:\n",
    "            self.sequence_stack.add_module(\"gru\", nn.GRU(input_size=hidden_size, hidden_size=hidden_size,\n",
    "                                                        num_layers=1, batch_first=True))\n",
    "        if \"RNN\" in ML_unit_type:\n",
    "            self.sequence_stack.add_module(\"rnn\", nn.RNN(input_size=hidden_size, hidden_size=hidden_size,\n",
    "                                                        num_layers=1, batch_first=True))\n",
    "        \n",
    "        \n",
    "\n",
    "        # Linear layer to map LSTM output to quantum layer input size\n",
    "        self.linear1 = nn.Linear(hidden_size, n_qubits)\n",
    "\n",
    "        # Optional quantum layer for variational feature extraction\n",
    "        if use_quantum:\n",
    "            self.quantum = QuantumLayer(n_qubits=n_qubits, q_depth=q_depth, n_rot_params=n_rot_params)\n",
    "\n",
    "        if self.use_layernorm:\n",
    "            self.layernorm = nn.LayerNorm(n_qubits)\n",
    "\n",
    "        if self.use_dropout:\n",
    "            self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "    \n",
    "        # ⬇ decide how many features go into final layer\n",
    "        if skip_connection == \"concat\":\n",
    "            in_features = 2 * n_qubits\n",
    "        else:\n",
    "            in_features = n_qubits\n",
    "\n",
    "        # Final linear layer\n",
    "        self.linear2 = nn.Linear(in_features, output_size)\n",
    "\n",
    "        # Final activation function for output layer\n",
    "        # to any of the supported activation functions\n",
    "        if output_activation == \"ReLU\":\n",
    "            self.output_activation = nn.ReLU()\n",
    "        elif output_activation == \"Tanh\":\n",
    "            self.output_activation = nn.Tanh()\n",
    "        elif output_activation == \"Sigmoid\":\n",
    "            self.output_activation = nn.Sigmoid()\n",
    "        elif output_activation == \"Softmax\":\n",
    "            self.output_activation = nn.Softmax(dim=1)\n",
    "        else:\n",
    "            self.output_activation = None\n",
    "            \n",
    "        self.output_activation_name = output_activation  # ← This keeps the original string\n",
    "\n",
    "           \n",
    "    def forward(self, x):\n",
    "        # Input shape check: should be (batch_size, sequence_length, input_size)\n",
    "        if len(x.shape) != 3:\n",
    "            raise ValueError(f\"Expected input shape (batch_size, seq_len, input_size), got {x.shape}\")\n",
    "\n",
    "        # Run input through sequence stack (LSTM/GRU/RNN)\n",
    "        rnn_out = x\n",
    "        for layer in self.sequence_stack:\n",
    "            if isinstance(layer, (nn.LSTM, nn.GRU, nn.RNN)):\n",
    "                rnn_out, _ = layer(rnn_out)\n",
    "            else:\n",
    "                rnn_out = layer(rnn_out)\n",
    "\n",
    "\n",
    "        # Use the final hidden state of the sequence\n",
    "        last_hidden = rnn_out[:, -1, :]\n",
    "\n",
    "        # Project to quantum input dimension\n",
    "        reduced = self.linear1(last_hidden)\n",
    "\n",
    "        # Apply quantum layer if enabled\n",
    "        if self.use_quantum:\n",
    "            quantum_out = self.quantum(reduced)\n",
    "            \n",
    "            if self.post_quantum_activation == \"ReLU\":\n",
    "                quantum_out = torch.relu(quantum_out)\n",
    "                \n",
    "            elif self.post_quantum_activation == \"Tanh\":\n",
    "                quantum_out = torch.tanh(quantum_out)\n",
    "                \n",
    "            elif self.post_quantum_activation == \"Sigmoid\":\n",
    "                quantum_out = torch.sigmoid(quantum_out)\n",
    "        \n",
    "        else:\n",
    "            quantum_out = reduced  # fallback to classical-only path\n",
    "            \n",
    "        # ✅ Optional regularization after quantum\n",
    "        if self.use_layernorm:\n",
    "            quantum_out = self.layernorm(quantum_out)\n",
    "\n",
    "        if self.use_dropout:\n",
    "            quantum_out = self.dropout(quantum_out)\n",
    "\n",
    "\n",
    "        # Concatenate classical and quantum outputs\n",
    "        if self.skip_connection == \"add\":\n",
    "            x_out = quantum_out + reduced\n",
    "        elif self.skip_connection == \"concat\":\n",
    "            x_out = torch.cat([quantum_out, reduced], dim=1)\n",
    "        else:\n",
    "            x_out = quantum_out  # quantum-only path\n",
    "\n",
    "        \n",
    "\n",
    "        # Final linear projection\n",
    "        out = self.linear2(x_out)\n",
    "\n",
    "        return self.output_activation(out) if self.output_activation else out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "230b597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)  # Shape: (batch, seq, 1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(-1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab91aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the sequence length based on the training data\n",
    "sequence_length = X_train.shape[1]\n",
    "\n",
    "# Instantiate the model \n",
    "# (use_quantum=True for quantum model, False for classical model)\n",
    "model = HybridQNN(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    n_qubits=n_qubits,\n",
    "    q_depth=q_depth,\n",
    "    n_rot_params=n_rot_params,\n",
    "    ML_unit_type=ML_unit_type,\n",
    "    num_ML_layers=num_ML_layers,\n",
    "    use_quantum=use_quantum,  # Set to True for quantum model\n",
    "    post_quantum_activation=post_quantum_activation,\n",
    "    skip_connection=skip_connection,\n",
    "    output_activation=output_activation,\n",
    "    use_dropout=use_dropout,\n",
    "    dropout_rate=dropout_rate,\n",
    "    use_layernorm=use_layernorm\n",
    ")\n",
    "\n",
    "#use GPU if available **TODO**\n",
    "    \n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3c4792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2ecc604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader_tqdm:\n\u001b[0;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 16\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, y_batch)\n\u001b[0;32m     18\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\npace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\npace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[10], line 87\u001b[0m, in \u001b[0;36mHybridQNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_stack:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, (nn\u001b[38;5;241m.\u001b[39mLSTM, nn\u001b[38;5;241m.\u001b[39mGRU, nn\u001b[38;5;241m.\u001b[39mRNN)):\n\u001b[1;32m---> 87\u001b[0m         rnn_out, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m         rnn_out \u001b[38;5;241m=\u001b[39m layer(rnn_out)\n",
      "File \u001b[1;32mc:\\Users\\npace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\npace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\npace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1124\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1121\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1138\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1146\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "# Use tqdm for progress bar in each epoch\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Use tqdm for progress bar in each epoch\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "    for X_batch, y_batch in train_loader_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # === Validation ===\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"📘 Epoch {epoch+1:3d} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "    # === Early Stopping Check ===\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Optional: save model checkpoint\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"⚠️  No improvement for {epochs_no_improve} epoch(s)\")\n",
    "\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(\"⛔ Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750968d",
   "metadata": {},
   "source": [
    "📈 Plot Training Loss\n",
    "Code Cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb90b678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded best model from checkpoint for final evaluation.\n"
     ]
    }
   ],
   "source": [
    "# === Load the best model for testing/prediction ===\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "print(\"✅ Loaded best model from checkpoint for final evaluation.\")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        predictions.append(y_pred.detach().cpu())\n",
    "\n",
    "# Concatenate and flatten\n",
    "predictions = torch.cat(predictions).squeeze().numpy()\n",
    "y_true = y_test_tensor.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd8976d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler data min: [24.73474693]\n",
      "Scaler data max: [196.43377686]\n"
     ]
    }
   ],
   "source": [
    "print(\"Scaler data min:\", scaler.data_min_)\n",
    "print(\"Scaler data max:\", scaler.data_max_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6745d9ec",
   "metadata": {},
   "source": [
    "**🔄 Inverse Transform (Denormalize)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abd4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for inverse_transform (must be 2D)\n",
    "predicted_prices = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "true_prices = scaler.inverse_transform(y_true.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe5d5bf",
   "metadata": {},
   "source": [
    "### 5.2 Evaluation Metrics\n",
    "\n",
    "We compute standard forecasting error metrics to quantify model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df1671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Test MAE  = 13.7149\n",
      "📈 Test RMSE = 18.4840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\npace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mae = mean_absolute_error(true_prices, predicted_prices)\n",
    "rmse = mean_squared_error(true_prices, predicted_prices, squared=False)\n",
    "\n",
    "print(f\"📈 Test MAE  = {mae:.4f}\")\n",
    "print(f\"📈 Test RMSE = {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab728f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Absolute Percentage Error: 8.41%\n"
     ]
    }
   ],
   "source": [
    "# Compute percentage errors\n",
    "percentage_errors = 100 * (predicted_prices - true_prices) / true_prices\n",
    "\n",
    "avg_percent_error = np.mean(np.abs(percentage_errors))\n",
    "print(f\"Average Absolute Percentage Error: {avg_percent_error:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b51d56",
   "metadata": {},
   "source": [
    "### 📊 Logging Expiremental Results to Excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f516e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_experiment(model, window_size, num_epochs, patience,\n",
    "                   avg_train_loss, avg_val_loss, mae, rmse,\n",
    "                   avg_percent_error, tickers, notes=\"\"):\n",
    "\n",
    "    # --- Extract model config ---\n",
    "    use_quantum = getattr(model, \"use_quantum\", False)\n",
    "    post_quantum_activation = getattr(model, \"post_quantum_activation\", None)\n",
    "    skip_connection = getattr(model, \"skip_connection\", None)\n",
    "    final_activation = getattr(model, \"output_activation_name\", None)\n",
    "    use_dropout = getattr(model, \"use_dropout\", False)\n",
    "    dropout_rate = getattr(model, \"dropout_rate\", None)\n",
    "    use_layernorm = getattr(model, \"use_layernorm\", False)\n",
    "\n",
    "    # --- Sequence architecture inspection ---\n",
    "    lstm_layers = 0\n",
    "    gru_layers = 0\n",
    "    rnn_layers = 0\n",
    "    hidden_size = None\n",
    "\n",
    "    for module in model.sequence_stack:\n",
    "        if isinstance(module, nn.LSTM):\n",
    "            lstm_layers += module.num_layers\n",
    "            hidden_size = module.hidden_size\n",
    "        elif isinstance(module, nn.GRU):\n",
    "            gru_layers += module.num_layers\n",
    "            hidden_size = module.hidden_size\n",
    "        elif isinstance(module, nn.RNN):\n",
    "            rnn_layers += module.num_layers\n",
    "            hidden_size = module.hidden_size\n",
    "\n",
    "    # --- Quantum circuit info (if used) ---\n",
    "    if use_quantum and hasattr(model, \"quantum\"):\n",
    "        n_qubits = getattr(model.quantum, \"n_qubits\", \"-\")\n",
    "        q_depth = getattr(model.quantum, \"q_depth\", \"-\")\n",
    "        n_rot_params = getattr(model.quantum, \"n_rot_params\", \"-\")\n",
    "    else:\n",
    "        n_qubits, q_depth, n_rot_params = \"-\", \"-\", \"-\"\n",
    "\n",
    "    # --- Load or initialize Excel sheet ---\n",
    "    try:\n",
    "        df = pd.read_excel(\"qml_experiment_log.xlsx\")\n",
    "        next_id = int(df[\"Experiment ID\"].max()) + 1\n",
    "    except (FileNotFoundError, ValueError, KeyError):\n",
    "        df = pd.DataFrame()\n",
    "        next_id = 1\n",
    "\n",
    "    # --- Assemble experiment entry ---\n",
    "    new_result = {\n",
    "        # 🧾 Metadata\n",
    "        \"Experiment ID\": next_id,\n",
    "        \"Tickers\": \", \".join(tickers),\n",
    "        \"Description\": f\"Auto-log: Q={n_qubits}, D={q_depth}, Skip={skip_connection}\",\n",
    "\n",
    "        # 🧠 Classical architecture\n",
    "        \"LSTM Layers\": lstm_layers,\n",
    "        \"GRU Layers\": gru_layers,\n",
    "        \"RNN Layers\": rnn_layers,\n",
    "        \"Hidden Size\": hidden_size,\n",
    "        \"Window Size\": window_size,\n",
    "\n",
    "        # ⚛️ Quantum architecture\n",
    "        \"Use Quantum\": use_quantum,\n",
    "        \"Qubits\": n_qubits,\n",
    "        \"Q Depth\": q_depth,\n",
    "        \"Rotation Params\": n_rot_params,\n",
    "        \"Skip Connection\": skip_connection,\n",
    "        \"Post-Quantum Activation\": post_quantum_activation,\n",
    "\n",
    "        # 🎛️ Regularization & Activation\n",
    "        \"Use Dropout\": use_dropout,\n",
    "        \"Dropout Rate\": dropout_rate,\n",
    "        \"Use LayerNorm\": use_layernorm,\n",
    "        \"Final Activation\": final_activation,\n",
    "\n",
    "        # 🏋️ Training setup\n",
    "        \"Num Epochs\": num_epochs,\n",
    "        \"Early Stop Patience\": patience,\n",
    "\n",
    "        # 📈 Results\n",
    "        \"Train Loss\": avg_train_loss,\n",
    "        \"Validation Loss\": avg_val_loss,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Avg % Error\": avg_percent_error,\n",
    "\n",
    "        # 🗒️ Notes\n",
    "        \"Notes\": notes\n",
    "    }\n",
    "\n",
    "    # --- Append and save ---\n",
    "    df = pd.concat([df, pd.DataFrame([new_result])], ignore_index=True)\n",
    "    df.to_excel(\"qml_experiment_log.xlsx\", index=False)\n",
    "    print(f\"📋 Experiment logged as ID {next_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74a31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Experiment logged as ID 1\n"
     ]
    }
   ],
   "source": [
    "log_experiment(\n",
    "    model=model,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=early_stop_patience,\n",
    "    avg_train_loss=avg_train_loss,\n",
    "    avg_val_loss=avg_val_loss,\n",
    "    mae=mae,\n",
    "    rmse=rmse,\n",
    "    avg_percent_error=avg_percent_error,\n",
    "    tickers=tickers,\n",
    "    notes=experiment_notes\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
