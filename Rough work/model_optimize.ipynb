{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b74a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchview import draw_graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66e9493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "# This is important for consistent results across runs\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # For GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Make operations deterministic (slower but reproducible)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e061c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading data for AAPL...\n",
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading data for MSFT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading data for GOOGL...\n",
      "✅ Loaded 3 tickers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tickers = [\"AAPL\", \"MSFT\", \"GOOGL\"]\n",
    "\n",
    "\n",
    "# ✅ Dictionary to hold processed DataFrames per ticker\n",
    "ticker_data = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"📥 Downloading data for {ticker}...\")\n",
    "    \n",
    "    df = yf.download(ticker, start=\"2015-01-01\", end=\"2024-12-31\", interval=\"1d\")\n",
    "\n",
    "    # Flatten MultiIndex if necessary (e.g., from yf multi-ticker download)\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "\n",
    "    # Focus on 'Close' prices\n",
    "    df = df[['Close']].copy()\n",
    "    df.rename(columns={'Close': 'price'}, inplace=True)\n",
    "\n",
    "    # Forward-fill missing values\n",
    "    df.ffill(inplace=True)\n",
    "\n",
    "    # Normalize prices with individual scalers\n",
    "    scaler = MinMaxScaler()\n",
    "    df['normalized'] = scaler.fit_transform(df[['price']])\n",
    "\n",
    "    # Store\n",
    "    ticker_data[ticker] = df\n",
    "\n",
    "print(f\"✅ Loaded {len(ticker_data)} tickers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5992f4",
   "metadata": {},
   "source": [
    "Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6768e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_notes = \"run with Quantum layer\"\n",
    "\n",
    "# Use a window size (e.g., 20)\n",
    "WINDOW_SIZE = 20\n",
    "\n",
    "# -------------------------------- Quantum Hyperparameters --------------------------------\n",
    "\n",
    "n_qubits = 2 # number of qubits\n",
    "q_depth  = 2 # number of layers\n",
    "n_rot_params = 3  # <--- number of rotation parameters per qubit (e.g. 1 for RY, 3 for Rot)\n",
    "\n",
    "# draw quantum circuit\n",
    "draw_qc_bool = True\n",
    "\n",
    "# wether to use quantum or classical model\n",
    "use_quantum = True\n",
    "\n",
    "# -------------------------------- Classical Hyperparameters --------------------------------\n",
    "\n",
    "# === Model Config ===\n",
    "# Set model hyperparameters\n",
    "input_size = 1\n",
    "hidden_size = 16\n",
    "output_size = 1\n",
    "\n",
    "# draw the QML or ML model graph\n",
    "draw_model_graph = True\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_epochs = 15\n",
    "use_dropout=False\n",
    "dropout_rate= 0.0 # 0.2 to 0.5 is common for LSTM/GRU\n",
    "use_layernorm=False \n",
    "\n",
    "# === Early Stopping Config ===\n",
    "early_stop_patience = 2 # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# TODO add ARIMA, xLSTM, and transformers options\n",
    "ML_unit_type = \"LSTM\"  # \"LSTM\", \"GRU\", or \"RNN\" can havy many layers\n",
    "num_ML_layers = 4  # Number of LSTM/GRU/RNN layers\n",
    "\n",
    "post_quantum_activation = None  # \"ReLU\", \"Tanh\", \"Sigmoid\", or None\n",
    "skip_connection = \"concat\"  # \"concat\" or \"add\"\n",
    "output_activation = None # \"ReLU\", \"Tanh\", \"Sigmoid\", \"Softmax\", or None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f32e192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creating sequences for AAPL...\n",
      "✅ AAPL: Train=(1996, 20), Test=(499, 20)\n",
      "🔄 Creating sequences for MSFT...\n",
      "✅ MSFT: Train=(1996, 20), Test=(499, 20)\n",
      "🔄 Creating sequences for GOOGL...\n",
      "✅ GOOGL: Train=(1996, 20), Test=(499, 20)\n",
      "\n",
      "📦 Combined Dataset — Train: (5988, 20), Test: (1497, 20)\n"
     ]
    }
   ],
   "source": [
    "# To perform time series forecasting, we convert the normalized series into input/output sequences \n",
    "# using a sliding window approach.\n",
    "# [past inputs] → [future targets]\n",
    "def create_sequences(series, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - window_size):\n",
    "        X.append(series[i:i + window_size])\n",
    "        y.append(series[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Initialize holders for aggregated data\n",
    "X_train_all, y_train_all = [], []\n",
    "X_test_all, y_test_all = [], []\n",
    "\n",
    "# Apply to each ticker\n",
    "for ticker, df in ticker_data.items():\n",
    "    print(f\"🔄 Creating sequences for {ticker}...\")\n",
    "    series = df['normalized'].values\n",
    "    X, y = create_sequences(series, WINDOW_SIZE)\n",
    "\n",
    "    # Time-respecting 80/20 split\n",
    "    split_index = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    # Append to global containers\n",
    "    X_train_all.append(X_train)\n",
    "    y_train_all.append(y_train)\n",
    "    X_test_all.append(X_test)\n",
    "    y_test_all.append(y_test)\n",
    "\n",
    "    print(f\"✅ {ticker}: Train={X_train.shape}, Test={X_test.shape}\")\n",
    "\n",
    "# Combine across all tickers\n",
    "X_train = np.vstack(X_train_all)\n",
    "y_train = np.hstack(y_train_all)\n",
    "X_test = np.vstack(X_test_all)\n",
    "y_test = np.hstack(y_test_all)\n",
    "\n",
    "print(f\"\\n📦 Combined Dataset — Train: {X_train.shape}, Test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4603e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device definition\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_circuit(inputs, weights, draw=False):\n",
    "    \"\"\"\n",
    "    Variational quantum circuit for hybrid model.\n",
    "    \n",
    "    Args:\n",
    "        inputs (Tensor): Input features (size ≤ n_qubits)\n",
    "        weights (Tensor): Trainable parameters of shape (q_depth, n_qubits, n_rot_params)\n",
    "        draw (bool): If True, draw the circuit once (ASCII + matplotlib)\n",
    "        \n",
    "    Returns:\n",
    "        List[Expectation values] for PauliZ on each qubit\n",
    "    \"\"\"\n",
    "    # --- Input Encoding ---\n",
    "    encoded_inputs = inputs[:n_qubits]\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(encoded_inputs[i], wires=i)\n",
    "    \n",
    "    # Optionally, use this instead:\n",
    "    # qml.templates.AngleEmbedding(inputs, wires=range(n_qubits), rotation='Y')\n",
    "\n",
    "    # --- Variational Layers ---\n",
    "    for layer in range(q_depth):\n",
    "        for i in range(n_qubits):\n",
    "            if n_rot_params == 1:\n",
    "                qml.RY(weights[layer][i][0], wires=i)\n",
    "            elif n_rot_params == 2:\n",
    "                qml.RX(weights[layer][i][0], wires=i)\n",
    "                qml.RZ(weights[layer][i][1], wires=i)\n",
    "            elif n_rot_params == 3:\n",
    "                qml.Rot(*weights[layer][i], wires=i)\n",
    "            else:\n",
    "                raise ValueError(\"n_rot_params must be 1, 2, or 3.\")\n",
    "\n",
    "        # Entanglement (ring topology)\n",
    "        for i in range(n_qubits):\n",
    "            qml.CNOT(wires=[i, (i + 1) % n_qubits])\n",
    "\n",
    "    # --- Optional Drawing ---\n",
    "    if draw:       \n",
    "        try:\n",
    "            fig, ax = qml.draw_mpl(quantum_circuit)(inputs, weights)\n",
    "            fig.tight_layout()\n",
    "            fig.show()\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ MPL circuit draw failed:\", e)\n",
    "\n",
    "    # --- Measurement ---\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "690f612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer(nn.Module):\n",
    "    def __init__(self, n_qubits, q_depth, n_rot_params):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize trainable parameters for the quantum circuit\n",
    "        # Shape: [q_depth, n_qubits, n_rot_params] (rotation angles per qubit per layer)\n",
    "        q_init = torch.empty(q_depth, n_qubits, n_rot_params)\n",
    "        torch.nn.init.normal_(q_init, mean=0.0, std=0.01)  # Small init to avoid flat gradients\n",
    "        self.q_params = nn.Parameter(q_init)\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.q_depth = q_depth\n",
    "        self.n_rot_params = n_rot_params\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        \"\"\"\n",
    "        Apply the quantum circuit to each sample in the batch.\n",
    "\n",
    "        Args:\n",
    "            x_batch (Tensor): Input of shape [batch_size, n_qubits]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output of shape [batch_size, n_qubits]\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        for x in x_batch:\n",
    "            # Apply the quantum circuit to each sample\n",
    "            q_out = quantum_circuit(x, self.q_params)\n",
    "\n",
    "            # Convert list of expectation values into a float32 tensor\n",
    "            q_tensor = torch.stack(q_out).to(dtype=torch.float32)\n",
    "\n",
    "            outputs.append(q_tensor)\n",
    "\n",
    "        # Stack the results into a batch tensor\n",
    "        return torch.stack(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08dba363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualQuantumLayer(nn.Module):\n",
    "    def __init__(self, out_features, in_features=None, label=\"QuantumLayer\", n_qubits=2, q_depth=1, n_rot_params=3):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.in_features = in_features or out_features\n",
    "\n",
    "        # 🧠 Add these to match real QuantumLayer\n",
    "        self.n_qubits = n_qubits\n",
    "        self.q_depth = q_depth\n",
    "        self.n_rot_params = n_rot_params\n",
    "\n",
    "        self.dummy = nn.Linear(self.in_features, self.out_features, bias=False)\n",
    "        for param in self.dummy.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dummy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3baf989",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridQNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                 n_qubits=4, q_depth=1, n_rot_params=3, ML_unit_type = 'LSTM', num_ML_layers = 1, use_quantum=True,\n",
    "                 post_quantum_activation=None, skip_connection=\"concat\",output_activation=\"Sigmoid\",\n",
    "                 use_dropout=False, dropout_rate=0.3,\n",
    "                 use_layernorm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_quantum = use_quantum\n",
    "        self.post_quantum_activation = post_quantum_activation  # 👈 Store this\n",
    "        self.skip_connection = skip_connection  # 👈 Store this\n",
    "        self.n_qubits = n_qubits  # 👈 Store this\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_layernorm = use_layernorm\n",
    "\n",
    "\n",
    "\n",
    "        ML_unit_type = ML_unit_type.upper()\n",
    "        # ✅ Check that it's at least valid before trying to build\n",
    "        if not any(unit in ML_unit_type for unit in [\"LSTM\", \"GRU\", \"RNN\"]):\n",
    "            raise ValueError(f\"Unsupported ML unit type: {ML_unit_type}. Supported types: LSTM, GRU, RNN\")\n",
    "\n",
    "        # Build stack\n",
    "        self.sequence_stack = nn.Sequential()\n",
    "\n",
    "        if \"LSTM\" in ML_unit_type:\n",
    "            self.sequence_stack.add_module(\"lstm\", nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                                                        num_layers=num_ML_layers, batch_first=True))\n",
    "\n",
    "        if \"GRU\" in ML_unit_type:\n",
    "            self.sequence_stack.add_module(\"gru\", nn.GRU(input_size=hidden_size, hidden_size=hidden_size,\n",
    "                                                        num_layers=1, batch_first=True))\n",
    "        if \"RNN\" in ML_unit_type:\n",
    "            self.sequence_stack.add_module(\"rnn\", nn.RNN(input_size=hidden_size, hidden_size=hidden_size,\n",
    "                                                        num_layers=1, batch_first=True))\n",
    "\n",
    "        # Linear layer to map LSTM output to quantum layer input size\n",
    "        self.linear1 = nn.Linear(hidden_size, n_qubits)\n",
    "\n",
    "        # Optional quantum layer for variational feature extraction\n",
    "        if use_quantum:\n",
    "            if torch.jit.is_scripting() or torch.jit.is_tracing():\n",
    "                # Use dummy version during tracing\n",
    "                self.quantum = VisualQuantumLayer(n_qubits)\n",
    "            else:\n",
    "                self.quantum = QuantumLayer(n_qubits=n_qubits, q_depth=q_depth, n_rot_params=n_rot_params)\n",
    "\n",
    "        if self.use_layernorm:\n",
    "            self.layernorm = nn.LayerNorm(n_qubits)\n",
    "\n",
    "        if self.use_dropout:\n",
    "            self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "    \n",
    "        # ⬇ decide how many features go into final layer\n",
    "        if skip_connection == \"concat\":\n",
    "            in_features = 2 * n_qubits\n",
    "        else:\n",
    "            in_features = n_qubits\n",
    "\n",
    "        # Final linear layer\n",
    "        self.linear2 = nn.Linear(in_features, output_size)\n",
    "\n",
    "        # Final activation function for output layer\n",
    "        # to any of the supported activation functions\n",
    "        if output_activation == \"ReLU\":\n",
    "            self.output_activation = nn.ReLU()\n",
    "        elif output_activation == \"Tanh\":\n",
    "            self.output_activation = nn.Tanh()\n",
    "        elif output_activation == \"Sigmoid\":\n",
    "            self.output_activation = nn.Sigmoid()\n",
    "        elif output_activation == \"Softmax\":\n",
    "            self.output_activation = nn.Softmax(dim=1)\n",
    "        else:\n",
    "            self.output_activation = None\n",
    "            \n",
    "        self.output_activation_name = output_activation  # ← This keeps the original string\n",
    "\n",
    "           \n",
    "    def forward(self, x):\n",
    "        # Input shape check: should be (batch_size, sequence_length, input_size)\n",
    "        if len(x.shape) != 3:\n",
    "            raise ValueError(f\"Expected input shape (batch_size, seq_len, input_size), got {x.shape}\")\n",
    "\n",
    "        # Run input through sequence stack (LSTM/GRU/RNN)\n",
    "        rnn_out = x\n",
    "        for layer in self.sequence_stack:\n",
    "            if isinstance(layer, (nn.LSTM, nn.GRU, nn.RNN)):\n",
    "                rnn_out, _ = layer(rnn_out)\n",
    "            else:\n",
    "                rnn_out = layer(rnn_out)\n",
    "\n",
    "\n",
    "        # Use the final hidden state of the sequence\n",
    "        last_hidden = rnn_out[:, -1, :]\n",
    "\n",
    "        # Project to quantum input dimension\n",
    "        reduced = self.linear1(last_hidden)\n",
    "\n",
    "        # Apply quantum layer if enabled\n",
    "        if self.use_quantum:\n",
    "            quantum_out = self.quantum(reduced)\n",
    "            \n",
    "            if self.post_quantum_activation == \"ReLU\":\n",
    "                quantum_out = torch.relu(quantum_out)\n",
    "                \n",
    "            elif self.post_quantum_activation == \"Tanh\":\n",
    "                quantum_out = torch.tanh(quantum_out)\n",
    "                \n",
    "            elif self.post_quantum_activation == \"Sigmoid\":\n",
    "                quantum_out = torch.sigmoid(quantum_out)\n",
    "        \n",
    "        else:\n",
    "            quantum_out = reduced  # fallback to classical-only path\n",
    "            \n",
    "        # ✅ Optional regularization after quantum\n",
    "        if self.use_layernorm:\n",
    "            quantum_out = self.layernorm(quantum_out)\n",
    "\n",
    "        if self.use_dropout:\n",
    "            quantum_out = self.dropout(quantum_out)\n",
    "\n",
    "\n",
    "        # Concatenate classical and quantum outputs\n",
    "        if self.skip_connection == \"add\":\n",
    "            x_out = quantum_out + reduced\n",
    "        elif self.skip_connection == \"concat\":\n",
    "            x_out = torch.cat([quantum_out, reduced], dim=1)\n",
    "        else:\n",
    "            x_out = quantum_out  # quantum-only path\n",
    "\n",
    "        \n",
    "\n",
    "        # Final linear projection\n",
    "        out = self.linear2(x_out)\n",
    "\n",
    "        return self.output_activation(out) if self.output_activation else out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3623021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)  # Shape: (batch, seq, 1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(-1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19e12e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the sequence length based on the training data\n",
    "sequence_length = X_train.shape[1]\n",
    "\n",
    "# Instantiate the model \n",
    "# (use_quantum=True for quantum model, False for classical model)\n",
    "model = HybridQNN(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_size,\n",
    "    n_qubits=n_qubits,\n",
    "    q_depth=q_depth,\n",
    "    n_rot_params=n_rot_params,\n",
    "    ML_unit_type=ML_unit_type,\n",
    "    num_ML_layers=num_ML_layers,\n",
    "    use_quantum=use_quantum,  # Set to True for quantum model\n",
    "    post_quantum_activation=post_quantum_activation,\n",
    "    skip_connection=skip_connection,\n",
    "    output_activation=output_activation,\n",
    "    use_dropout=use_dropout,\n",
    "    dropout_rate=dropout_rate,\n",
    "    use_layernorm=use_layernorm\n",
    ")\n",
    "\n",
    "#use GPU if available **TODO**\n",
    "    \n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b9e5a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch   1 | Train Loss: 0.073029 | Val Loss: 0.046277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch   2 | Train Loss: 0.001114 | Val Loss: 0.014332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch   3 | Train Loss: 0.000475 | Val Loss: 0.009470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch   4 | Train Loss: 0.000408 | Val Loss: 0.010026\n",
      "⚠️  No improvement for 1 epoch(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Epoch   5 | Train Loss: 0.000399 | Val Loss: 0.012006\n",
      "⚠️  No improvement for 2 epoch(s)\n",
      "⛔ Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# Use tqdm for progress bar in each epoch\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Use tqdm for progress bar in each epoch\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "    for X_batch, y_batch in train_loader_tqdm:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # === Validation ===\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"📘 Epoch {epoch+1:3d} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "    # === Early Stopping Check ===\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Optional: save model checkpoint\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"⚠️  No improvement for {epochs_no_improve} epoch(s)\")\n",
    "\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(\"⛔ Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e88495b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded best model from checkpoint for final evaluation.\n"
     ]
    }
   ],
   "source": [
    "# === Load the best model for testing/prediction ===\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "print(\"✅ Loaded best model from checkpoint for final evaluation.\")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, _ in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        predictions.append(y_pred.detach().cpu())\n",
    "\n",
    "# Concatenate and flatten\n",
    "predictions = torch.cat(predictions).squeeze().numpy()\n",
    "y_true = y_test_tensor.squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8b65716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for inverse_transform (must be 2D)\n",
    "predicted_prices = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "true_prices = scaler.inverse_transform(y_true.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4a5524c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤 Collecting results for AAPL...\n",
      "📤 Collecting results for MSFT...\n",
      "📤 Collecting results for GOOGL...\n"
     ]
    }
   ],
   "source": [
    "# Create storage dict\n",
    "ticker_sequences = {}\n",
    "\n",
    "# Index tracking for slicing predictions back per ticker\n",
    "test_start = 0\n",
    "\n",
    "for ticker, df in ticker_data.items():\n",
    "    print(f\"📤 Collecting results for {ticker}...\")\n",
    "\n",
    "    # Recreate y_test length for this ticker\n",
    "    series = df['normalized'].values\n",
    "    X_all, y_all = create_sequences(series, WINDOW_SIZE)\n",
    "    split_index = int(len(X_all) * 0.8)\n",
    "    y_test_len = len(y_all[split_index:])\n",
    "\n",
    "    # Slice predictions and ground truth\n",
    "    ticker_pred = predicted_prices[test_start:test_start + y_test_len]\n",
    "    ticker_true = true_prices[test_start:test_start + y_test_len]\n",
    "\n",
    "    # Save to dictionary\n",
    "    ticker_sequences[ticker] = {\n",
    "        \"predicted_prices\": ticker_pred,\n",
    "        \"true_prices\": ticker_true,\n",
    "        \"dates\": df.index[-y_test_len:]  # Match test dates\n",
    "    }\n",
    "\n",
    "    test_start += y_test_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0df2e117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Test MAE  = 12.2465\n",
      "📈 Test RMSE = 16.5867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\npace\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mae = mean_absolute_error(true_prices, predicted_prices)\n",
    "rmse = mean_squared_error(true_prices, predicted_prices, squared=False)\n",
    "\n",
    "print(f\"📈 Test MAE  = {mae:.4f}\")\n",
    "print(f\"📈 Test RMSE = {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "447b3d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Absolute Percentage Error: 7.47%\n"
     ]
    }
   ],
   "source": [
    "# Compute percentage errors\n",
    "percentage_errors = 100 * (predicted_prices - true_prices) / true_prices\n",
    "\n",
    "avg_percent_error = np.mean(np.abs(percentage_errors))\n",
    "print(f\"Average Absolute Percentage Error: {avg_percent_error:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf1d85e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_experiment(model, window_size, num_epochs, patience,\n",
    "                   avg_train_loss, avg_val_loss, mae, rmse,\n",
    "                   avg_percent_error, tickers, notes=\"\"):\n",
    "\n",
    "    # --- Extract model config ---\n",
    "    use_quantum = getattr(model, \"use_quantum\", False)\n",
    "    post_quantum_activation = getattr(model, \"post_quantum_activation\", None)\n",
    "    skip_connection = getattr(model, \"skip_connection\", None)\n",
    "    final_activation = getattr(model, \"output_activation_name\", None)\n",
    "    use_dropout = getattr(model, \"use_dropout\", False)\n",
    "    dropout_rate = getattr(model, \"dropout_rate\", None)\n",
    "    use_layernorm = getattr(model, \"use_layernorm\", False)\n",
    "\n",
    "    # --- Sequence architecture inspection ---\n",
    "    lstm_layers = 0\n",
    "    gru_layers = 0\n",
    "    rnn_layers = 0\n",
    "    hidden_size = None\n",
    "\n",
    "    for module in model.sequence_stack:\n",
    "        if isinstance(module, nn.LSTM):\n",
    "            lstm_layers += module.num_layers\n",
    "            hidden_size = module.hidden_size\n",
    "        elif isinstance(module, nn.GRU):\n",
    "            gru_layers += module.num_layers\n",
    "            hidden_size = module.hidden_size\n",
    "        elif isinstance(module, nn.RNN):\n",
    "            rnn_layers += module.num_layers\n",
    "            hidden_size = module.hidden_size\n",
    "\n",
    "    # --- Quantum circuit info (if used) ---\n",
    "    if use_quantum and hasattr(model, \"quantum\"):\n",
    "        n_qubits = getattr(model.quantum, \"n_qubits\", \"-\")\n",
    "        q_depth = getattr(model.quantum, \"q_depth\", \"-\")\n",
    "        n_rot_params = getattr(model.quantum, \"n_rot_params\", \"-\")\n",
    "    else:\n",
    "        n_qubits, q_depth, n_rot_params = \"-\", \"-\", \"-\"\n",
    "\n",
    "    # --- Load or initialize Excel sheet ---\n",
    "    try:\n",
    "        df = pd.read_excel(\"qml_experiment_log.xlsx\")\n",
    "        next_id = int(df[\"Experiment ID\"].max()) + 1\n",
    "    except (FileNotFoundError, ValueError, KeyError):\n",
    "        df = pd.DataFrame()\n",
    "        next_id = 1\n",
    "\n",
    "    # --- Assemble experiment entry ---\n",
    "    new_result = {\n",
    "        # 🧾 Metadata\n",
    "        \"Experiment ID\": next_id,\n",
    "        \"Tickers\": \", \".join(tickers),\n",
    "        \"Description\": f\"Auto-log: Q={n_qubits}, D={q_depth}, Skip={skip_connection}\",\n",
    "\n",
    "        # 🧠 Classical architecture\n",
    "        \"LSTM Layers\": lstm_layers,\n",
    "        \"GRU Layers\": gru_layers,\n",
    "        \"RNN Layers\": rnn_layers,\n",
    "        \"Hidden Size\": hidden_size,\n",
    "        \"Window Size\": window_size,\n",
    "\n",
    "        # ⚛️ Quantum architecture\n",
    "        \"Use Quantum\": use_quantum,\n",
    "        \"Qubits\": n_qubits,\n",
    "        \"Q Depth\": q_depth,\n",
    "        \"Rotation Params\": n_rot_params,\n",
    "        \"Skip Connection\": skip_connection,\n",
    "        \"Post-Quantum Activation\": post_quantum_activation,\n",
    "\n",
    "        # 🎛️ Regularization & Activation\n",
    "        \"Use Dropout\": use_dropout,\n",
    "        \"Dropout Rate\": dropout_rate,\n",
    "        \"Use LayerNorm\": use_layernorm,\n",
    "        \"Final Activation\": final_activation,\n",
    "\n",
    "        # 🏋️ Training setup\n",
    "        \"Num Epochs\": num_epochs,\n",
    "        \"Early Stop Patience\": patience,\n",
    "\n",
    "        # 📈 Results\n",
    "        \"Train Loss\": avg_train_loss,\n",
    "        \"Validation Loss\": avg_val_loss,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Avg % Error\": avg_percent_error,\n",
    "\n",
    "        # 🗒️ Notes\n",
    "        \"Notes\": notes\n",
    "    }\n",
    "\n",
    "    # --- Append and save ---\n",
    "    df = pd.concat([df, pd.DataFrame([new_result])], ignore_index=True)\n",
    "    df.to_excel(\"qml_experiment_log.xlsx\", index=False)\n",
    "    print(f\"📋 Experiment logged as ID {next_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fae25ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Experiment logged as ID 1\n"
     ]
    }
   ],
   "source": [
    "log_experiment(\n",
    "    model=model,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=early_stop_patience,\n",
    "    avg_train_loss=avg_train_loss,\n",
    "    avg_val_loss=avg_val_loss,\n",
    "    mae=mae,\n",
    "    rmse=rmse,\n",
    "    avg_percent_error=avg_percent_error,\n",
    "    tickers=tickers,\n",
    "    notes=experiment_notes\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
